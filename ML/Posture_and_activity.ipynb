{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2318,
     "status": "ok",
     "timestamp": 1696982674356,
     "user": {
      "displayName": "Jonathan Mui Koy Kit",
      "userId": "02155718658179568681"
     },
     "user_tz": -480
    },
    "id": "DXgKdd0ZCJnm"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "# import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Label and One-Hot encode the data\n",
    "Some images could have multiple labels (i.e. someone could be using the computer and have bad posture). The following code goes through the entire folder and creates a csv file with the image path and one-hot encoded labels of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_name</th>\n",
       "      <th>computer_use</th>\n",
       "      <th>asleep</th>\n",
       "      <th>reading</th>\n",
       "      <th>not_present</th>\n",
       "      <th>good_posture</th>\n",
       "      <th>bad_posture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/images/reading/rgp2 (2).jpg</td>\n",
       "      <td>rgp2 (2).jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/images/reading/rbp1 (30).jpg</td>\n",
       "      <td>rbp1 (30).jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/images/reading/rgp2 (14).jpg</td>\n",
       "      <td>rgp2 (14).jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/images/reading/rgp1 (4).jpg</td>\n",
       "      <td>rgp1 (4).jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/images/reading/rgp3 (14).jpg</td>\n",
       "      <td>rgp3 (14).jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>dataset/images/computer_use/busy-work-laptop-s...</td>\n",
       "      <td>busy-work-laptop-sits-large-empty-table-bright...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>dataset/images/computer_use/20231015122142.jpg</td>\n",
       "      <td>20231015122142.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>dataset/images/computer_use/20231015122817.jpg</td>\n",
       "      <td>20231015122817.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>dataset/images/computer_use/istockphoto-164929...</td>\n",
       "      <td>istockphoto-1649292491-1024x1024.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>dataset/images/computer_use/istockphoto-103139...</td>\n",
       "      <td>istockphoto-1031398222-1024x1024.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path  \\\n",
       "0                  dataset/images/reading/rgp2 (2).jpg   \n",
       "1                 dataset/images/reading/rbp1 (30).jpg   \n",
       "2                 dataset/images/reading/rgp2 (14).jpg   \n",
       "3                  dataset/images/reading/rgp1 (4).jpg   \n",
       "4                 dataset/images/reading/rgp3 (14).jpg   \n",
       "..                                                 ...   \n",
       "856  dataset/images/computer_use/busy-work-laptop-s...   \n",
       "875     dataset/images/computer_use/20231015122142.jpg   \n",
       "876     dataset/images/computer_use/20231015122817.jpg   \n",
       "887  dataset/images/computer_use/istockphoto-164929...   \n",
       "899  dataset/images/computer_use/istockphoto-103139...   \n",
       "\n",
       "                                            image_name computer_use asleep  \\\n",
       "0                                         rgp2 (2).jpg            0      0   \n",
       "1                                        rbp1 (30).jpg            0      0   \n",
       "2                                        rgp2 (14).jpg            0      0   \n",
       "3                                         rgp1 (4).jpg            0      0   \n",
       "4                                        rgp3 (14).jpg            0      0   \n",
       "..                                                 ...          ...    ...   \n",
       "856  busy-work-laptop-sits-large-empty-table-bright...            1      0   \n",
       "875                                 20231015122142.jpg            1      0   \n",
       "876                                 20231015122817.jpg            1      0   \n",
       "887               istockphoto-1649292491-1024x1024.jpg            1      0   \n",
       "899               istockphoto-1031398222-1024x1024.jpg            1      0   \n",
       "\n",
       "    reading not_present good_posture bad_posture  \n",
       "0         1           0            1           0  \n",
       "1         1           0            0           1  \n",
       "2         1           0            1           0  \n",
       "3         1           0            1           0  \n",
       "4         1           0            1           0  \n",
       "..      ...         ...          ...         ...  \n",
       "856       0           0            0           0  \n",
       "875       0           0            0           0  \n",
       "876       0           0            0           0  \n",
       "887       0           0            0           0  \n",
       "899       0           0            0           0  \n",
       "\n",
       "[522 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the dataset directory and labels\n",
    "dataset_dir = 'dataset/images'\n",
    "labels = ['computer_use', 'asleep', 'reading', 'not_present', 'good_posture', 'bad_posture']\n",
    "\n",
    "# Create an empty dataframe with columns for image paths, image names, and labels\n",
    "df = pd.DataFrame(columns=['image_path', 'image_name'] + labels)\n",
    "\n",
    "# Traverse the dataset directory\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(('.png', '.jpg', '.jpeg', '.webp')):  # Check for image files\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_name = os.path.basename(image_path)\n",
    "            \n",
    "            # Create a row with default label values set to 0\n",
    "            row = {'image_path': image_path, 'image_name': image_name}\n",
    "            for label in labels:\n",
    "                row[label] = 0\n",
    "            \n",
    "            # Check the parent folder name and set the corresponding label to 1\n",
    "            parent_folder = os.path.basename(root)\n",
    "            if parent_folder in labels:\n",
    "                row[parent_folder] = 1\n",
    "            \n",
    "            # Append the row to the dataframe\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Filter the dataframe to only include rows with duplicate image_name values\n",
    "duplicates = df[df.duplicated('image_name', keep=False)]\n",
    "\n",
    "# Sort the duplicates dataframe based on image_name\n",
    "duplicates = duplicates.sort_values(by='image_name')\n",
    "\n",
    "# Extract the root folders for each duplicate image_name\n",
    "duplicates['root_folder'] = duplicates['image_path'].apply(lambda x: os.path.basename(os.path.dirname(x)))\n",
    "\n",
    "# Group by image_name and aggregate the labels\n",
    "aggregated_labels = duplicates.groupby('image_name')[labels].sum().clip(upper=1)\n",
    "\n",
    "# Update the original dataframe with the aggregated labels\n",
    "for image_name, row in aggregated_labels.iterrows():\n",
    "    mask = df['image_name'] == image_name\n",
    "    for label in labels:\n",
    "        df.loc[mask, label] = row[label]\n",
    "\n",
    "# Remove duplicates based on image_name, keeping only the first occurrence\n",
    "df = df.drop_duplicates(subset='image_name', keep='first')\n",
    "\n",
    "df.to_csv('dataset/labels.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8lbuLis-JYS"
   },
   "source": [
    "# 2. Preprocess the images & load the data\n",
    "This involves resizing the images to a consistent size, converting them to grayscale, reducing noise and normalizing the pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 0.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom dataset class to load the images and one-hot encoded labels from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx, 0]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        labels = self.dataframe.iloc[idx, 2:].values.astype(float) # gets label columns and converts to float\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomDataset(csv_file='dataset/labels.csv', transform=transform)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 128, 128]) torch.Size([32, 6])\n",
      "Labels:  tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Check data loader \n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)\n",
    "print(\"Labels: \", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a CNN\n",
    "Creating a convolutional neural network (CNN) for a multi-label image classification problem using PyTorch involves defining a model architecture that ends with a sigmoid activation function for each label, rather than a single softmax layer as in multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=16384, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MultiLabelCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 512)  # Input image size is 128x128\n",
    "        self.fc2 = nn.Linear(512, 6)  # 6 labels in your dataset\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for multi-label classification\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiLabelCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the model\n",
    "When training the model, use the BCEWithLogitsLoss as the loss function. This loss function is designed for multi-label classification and combines the sigmoid activation function with binary cross-entropy loss.\n",
    "\n",
    "During training, compute the loss using the predicted outputs and the true labels, and then backpropagate the errors to update the model's weights.\n",
    "\n",
    "Remember, in multi-label classification, an image can belong to multiple classes simultaneously. The model will output a value between 0 and 1 for each label, indicating the probability of the image belonging to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='mps:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0., 1.]], device='mps:0')\n",
      "Epoch [1/5] - Loss: 0.1174 - Accuracy: 0.5987\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='mps:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0., 1.]], device='mps:0')\n",
      "Epoch [2/5] - Loss: 0.1155 - Accuracy: 0.7094\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='mps:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0., 1.]], device='mps:0')\n",
      "Epoch [3/5] - Loss: 0.1155 - Accuracy: 0.7094\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='mps:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0., 1.]], device='mps:0')\n",
      "Epoch [4/5] - Loss: 0.1155 - Accuracy: 0.7094\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='mps:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0., 1.]], device='mps:0')\n",
      "Epoch [5/5] - Loss: 0.1155 - Accuracy: 0.7094\n"
     ]
    }
   ],
   "source": [
    "# define training hyperparameters\n",
    "lr = 1e-3\n",
    "num_epochs = 5\n",
    "\n",
    "# set the device we will be using to train the model (to enable hardware acceleration)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # setting for mac\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        # Shift data to device as well\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Compute predicted labels\n",
    "        predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        \n",
    "        # Update number of correct predictions\n",
    "        correct_preds += (predicted_labels == labels).sum().item()\n",
    "        \n",
    "        # Update total samples\n",
    "        total_samples += labels.numel()\n",
    "    \n",
    "    # Compute average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = correct_preds / total_samples\n",
    "    print(torch.sigmoid(outputs))\n",
    "    print(labels)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]\n",
      " [False False False False False False]]\n",
      "[[0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "computer_use       0.00      0.00      0.00        43\n",
      "      asleep       0.00      0.00      0.00        13\n",
      "     reading       0.00      0.00      0.00        34\n",
      " not_present       0.00      0.00      0.00        24\n",
      "good_posture       0.00      0.00      0.00        36\n",
      " bad_posture       0.00      0.00      0.00        34\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       184\n",
      "   macro avg       0.00      0.00      0.00       184\n",
      "weighted avg       0.00      0.00      0.00       184\n",
      " samples avg       0.00      0.00      0.00       184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/cv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "label_names = [\"computer_use\", \"asleep\", \"reading\", \"not_present\", \"good_posture\", \"bad_posture\"]\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5) # Apply sigmoid and threshold at 0.5\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate results from all batches\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "print(all_preds)\n",
    "print(all_labels)\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "binary_preds = all_preds.astype(float)\n",
    "\n",
    "print(classification_report(all_labels, binary_preds, target_names=label_names))  # Assuming label_names is a list of your label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN76HAm9Ury5zS6u3sgjirA",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
